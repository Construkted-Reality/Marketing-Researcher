# OpenAI API configuration (for vLLM server compatibility)
# Set OPENAI_API_KEY to a placeholder or actual key if your vLLM server requires it for authentication,
# even if it's a local server. If your local vLLM server doesn't require an API key, you can leave it blank.

# =============================================================================
# Python Script LLM configuration (used to parse the insights into JSON)
# =============================================================================
OPENAI_API_BASE=http://192.168.8.90:42069/v1
OPENAI_API_KEY=outsider
OPENAI_MODEL_NAME=gpt-oss-120b
#OPENAI_MODEL_NAME=qwen3-coder-30b-awq8



# Tavily API key (if you plan to use Tavily for research)
#TAVILY_API_KEY= 

# SearXNG URL for self-hosted search engine
SEARX_URL=https://search.roci.me/

# =============================================================================
# GPTresearcher configuration (using new configuration format)
# =============================================================================

VLLM_OPENAI_API_BASE=http://192.168.8.90:42069/v1
VLLM_OPENAI_API_KEY=outsider

FAST_LLM=vllm_openai:gpt-oss-120b
SMART_LLM=vllm_openai:gpt-oss-120b
STRATEGIC_LLM=vllm_openai:gpt-oss-120b

TEMPERATURE=0.7

# GPT 5 cost
LLM_IN_PRICE_PER_1K=.000125 
LLM_OUT_PRICE_PER_1K=.01

# oss-120b on open router cost
#LLM_IN_PRICE_PER_1K=.000125 
#LLM_OUT_PRICE_PER_1K=.01

# =============================================================================
# SEARCH ENGINE CONFIGURATION - Choose ONE option below
# =============================================================================

# OPTION 1: SearXNG + MCP Hybrid (Default)
# Privacy-first self-hosted search with AI reasoning
RETRIEVER=searx,mcp

# OPTION 2: Tavily + MCP Hybrid (Backup)
# Commercial search API with AI reasoning
#RETRIEVER=tavily,mcp

# OPTION 3: SearXNG Only
# Pure self-hosted search, no AI reasoning
# RETRIEVER=searx

# OPTION 4: Tavily Only
# Pure commercial search API, no AI reasoning
# RETRIEVER=tavily

# =============================================================================

# Web Scraping Configuration
SCRAPER=bs  # Options: bs (BeautifulSoup), browser (Selenium), tavily_extract, firecrawl

# Github Personal Access Token (if using github mcp server)
# GITHUB_PERSONAL_ACCESS_TOKEN=your_github_token

#SEARCH_PROVIDER=tavily  # Options: tavily, serpapi, google

# =============================================================================
# EMBEDDING CONFIGURATION - Choose ONE option below
# =============================================================================

# OPTION 1: Ollama Server on LAN (Currently Active)
# Fast, runs on dedicated server, good for distributed setups
#EMBEDDING=ollama:mxbai-embed-large
EMBEDDING=ollama:embeddinggemma-300m-qat-q8_0
OLLAMA_BASE_URL=http://192.168.8.185:11434

# OPTION 2: Local HuggingFace Sentence Transformers (Backup)
# Slower, runs locally, good for single-machine setups
# EMBEDDING=huggingface:sentence-transformers/all-MiniLM-L6-v2

# OPTION 3: Alternative Ollama Models (if available on your server)
# EMBEDDING=ollama:nomic-embed-text

# OPTION 4: Different Server IP Example
# EMBEDDING=ollama:mxbai-embed-large
# OLLAMA_BASE_URL=http://192.168.8.91:11434

# =============================================================================

# Legacy settings (deprecated but keeping for compatibility)
#LLM_PROVIDER=openai  # Options: openai, local_openai, google, anthropic

# -------------------------------------------------------------------------
# RESEARCH DEPTH / WEBSITE LIMIT SETTINGS (add or adjust as needed)
# -------------------------------------------------------------------------
MAX_SEARCH_RESULTS_PER_QUERY=25          # How many URLs to fetch per query | Default:5, typical range: 1-20
MAX_ITERATIONS=4                         # Number of refinement loops (depth) | Default:3, typical range: 1-5 (6 for very deep runs)
#DEEP_RESEARCH_BREADTH=7                  # Parallel research paths | Default:4, typical range: 1-8
#DEEP_RESEARCH_DEPTH=3                    # Levels of recursion per path | Default:2, typical range: 1-4
#DEEP_RESEARCH_CONCURRENCY=4              # Async concurrency for deep research | Default:4, typical range: 1-16
#TOTAL_WORDS=4500                         # Target length of final report | Default: 1200, typical range: any
#SIMILARITY_THRESHOLD=0.5                 # Filter low‑relevance docs | Default:0.42, typical range: 0.2-0.8
#MAX_SUBTOPICS=5                          # Max sub‑topics generated | Default:3, typical range: 1-10

BROWSE_CHUNK_MAX_LENGTH=32768
FAST_TOKEN_LIMIT=128000
SMART_TOKEN_LIMIT=128000

CURATE_SOURCES=true                      # Whether to curate sources for research. This step adds an LLM run which may increase costs and total run time but improves quality of source selection. Defaults to False.
REASONING_EFFORT=high                    # Controls the reasoning effort of strategic models. Default to medium.
