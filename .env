# OpenAI API configuration (for vLLM server compatibility)
# Set OPENAI_API_KEY to a placeholder or actual key if your vLLM server requires it for authentication,
# even if it's a local server. If your local vLLM server doesn't require an API key, you can leave it blank.
OPENAI_API_KEY=outsider

# Base URL for the local vLLM server
OPENAI_API_BASE=http://192.168.8.90:42069/v1

# Model name to use with the vLLM server
OPENAI_MODEL_NAME=gpt-oss-120b
#OPENAI_MODEL_NAME=qwen3-coder-30b-awq8

# Tavily API key (if you plan to use Tavily for research)
TAVILY_API_KEY=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

# SearXNG URL for self-hosted search engine
SEARX_URL=https://search.roci.me/

# =============================================================================
# SEARCH ENGINE CONFIGURATION - Choose ONE option below
# =============================================================================

# OPTION 1: SearXNG + MCP Hybrid (Default)
# Privacy-first self-hosted search with AI reasoning
RETRIEVER=searx,mcp

# OPTION 2: Tavily + MCP Hybrid (Backup)
# Commercial search API with AI reasoning
#RETRIEVER=tavily,mcp

# OPTION 3: SearXNG Only
# Pure self-hosted search, no AI reasoning
# RETRIEVER=searx

# OPTION 4: Tavily Only
# Pure commercial search API, no AI reasoning
# RETRIEVER=tavily

# =============================================================================

# Web Scraping Configuration
SCRAPER=bs  # Options: bs (BeautifulSoup), browser (Selenium), tavily_extract, firecrawl

# Github Personal Access Token (if using github mcp server)
# GITHUB_PERSONAL_ACCESS_TOKEN=your_github_token

# GPT Researcher Settings (using new configuration format)
FAST_LLM=vllm_openai:gpt-oss-120b
SMART_LLM=vllm_openai:gpt-oss-120b
SEARCH_PROVIDER=tavily  # Options: tavily, serpapi, google

# =============================================================================
# EMBEDDING CONFIGURATION - Choose ONE option below
# =============================================================================

# OPTION 1: Ollama Server on LAN (Currently Active)
# Fast, runs on dedicated server, good for distributed setups
EMBEDDING=ollama:mxbai-embed-large
OLLAMA_BASE_URL=http://192.168.8.90:11434

# OPTION 2: Local HuggingFace Sentence Transformers (Backup)
# Slower, runs locally, good for single-machine setups
# EMBEDDING=huggingface:sentence-transformers/all-MiniLM-L6-v2

# OPTION 3: Alternative Ollama Models (if available on your server)
# EMBEDDING=ollama:nomic-embed-text

# OPTION 4: Different Server IP Example
# EMBEDDING=ollama:mxbai-embed-large
# OLLAMA_BASE_URL=http://192.168.8.91:11434

# =============================================================================

# Legacy settings (deprecated but keeping for compatibility)
LLM_PROVIDER=openai  # Options: openai, local_openai, google, anthropic

# -------------------------------------------------------------------------
# RESEARCH DEPTH / WEBSITE LIMIT SETTINGS (add or adjust as needed)
# -------------------------------------------------------------------------
#MAX_SEARCH_RESULTS_PER_QUERY=25          # How many URLs to fetch per query | Default:5, typical range: 1-20
#MAX_ITERATIONS=4                         # Number of refinement loops (depth) | Default:3, typical range: 1-5
#DEEP_RESEARCH_BREADTH=7                  # Parallel research paths | Default:4, typical range: 1-8
#DEEP_RESEARCH_DEPTH=3                    # Levels of recursion per path | Default:2, typical range: 1-4
#DEEP_RESEARCH_CONCURRENCY=4              # Async concurrency for deep research | Default:4, typical range: 1-16
#TOTAL_WORDS=4500                         # Target length of final report | Default: 1200, typical range: any
#SIMILARITY_THRESHOLD=0.5                 # Filter low‑relevance docs | Default:0.42, typical range: 0.2-0.8
#MAX_SUBTOPICS=5                          # Max sub‑topics generated | Default:3, typical range: 1-10


